{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35fce833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading filtered dataset...\n",
      "Total records available: 476443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maireg\\AppData\\Local\\Temp\\ipykernel_9336\\1764979292.py:25: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = df.groupby('Unified_Product', group_keys=False).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stratified Sample Size: 12000\n",
      "Unified_Product\n",
      "Credit Card        3000\n",
      "Money Transfers    3000\n",
      "Personal Loan      3000\n",
      "Savings Account    3000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Preparing documents...\n",
      "Splitting 12000 documents into chunks...\n",
      "Total chunks created: 34897\n",
      "\n",
      "Initializing Embedding Model (all-MiniLM-L6-v2)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b484525aa7d4b20bd64dd61e9ffd653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Maireg\\Documents\\GitHub\\creditrust-complaint-rag\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Maireg\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b9c3b93bd54a008c7f561f910f453a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7eeb999306c4ab689c7329e77b8ef90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9cd5c970c8498d939e93e9dde6381c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4b3934ed8046e6bd963b65fe98c06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24daedaa33e44b38020f53b3dc58592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582b76a7404140e39e56aee8c6a120d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e419d168b0ed4fbaaa0a9ca9f2f7a602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff648e462814554b44fdd0912aaa40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7872c354fec04a7199f3bec1fc1bf514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5473e878ec2e4b4b8b3a29e8b1df5a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Chroma Vector Store at ../vector_store...\n",
      "This may take 5-10 minutes depending on your CPU. Please wait...\n",
      "Processed batch 0/34897...\n",
      "Processed batch 500/34897...\n",
      "Processed batch 1000/34897...\n",
      "Processed batch 1500/34897...\n",
      "Processed batch 2000/34897...\n",
      "Processed batch 2500/34897...\n",
      "Processed batch 3000/34897...\n",
      "Processed batch 3500/34897...\n",
      "Processed batch 4000/34897...\n",
      "Processed batch 4500/34897...\n",
      "Processed batch 5000/34897...\n",
      "Processed batch 5500/34897...\n",
      "Processed batch 6000/34897...\n",
      "Processed batch 6500/34897...\n",
      "Processed batch 7000/34897...\n",
      "Processed batch 7500/34897...\n",
      "Processed batch 8000/34897...\n",
      "Processed batch 8500/34897...\n",
      "Processed batch 9000/34897...\n",
      "Processed batch 9500/34897...\n",
      "Processed batch 10000/34897...\n",
      "Processed batch 10500/34897...\n",
      "Processed batch 11000/34897...\n",
      "Processed batch 11500/34897...\n",
      "Processed batch 12000/34897...\n",
      "Processed batch 12500/34897...\n",
      "Processed batch 13000/34897...\n",
      "Processed batch 13500/34897...\n",
      "Processed batch 14000/34897...\n",
      "Processed batch 14500/34897...\n",
      "Processed batch 15000/34897...\n",
      "Processed batch 15500/34897...\n",
      "Processed batch 16000/34897...\n",
      "Processed batch 16500/34897...\n",
      "Processed batch 17000/34897...\n",
      "Processed batch 17500/34897...\n",
      "Processed batch 18000/34897...\n",
      "Processed batch 18500/34897...\n",
      "Processed batch 19000/34897...\n",
      "Processed batch 19500/34897...\n",
      "Processed batch 20000/34897...\n",
      "Processed batch 20500/34897...\n",
      "Processed batch 21000/34897...\n",
      "Processed batch 21500/34897...\n",
      "Processed batch 22000/34897...\n",
      "Processed batch 22500/34897...\n",
      "Processed batch 23000/34897...\n",
      "Processed batch 23500/34897...\n",
      "Processed batch 24000/34897...\n",
      "Processed batch 24500/34897...\n",
      "Processed batch 25000/34897...\n",
      "Processed batch 25500/34897...\n",
      "Processed batch 26000/34897...\n",
      "Processed batch 26500/34897...\n",
      "Processed batch 27000/34897...\n",
      "Processed batch 27500/34897...\n",
      "Processed batch 28000/34897...\n",
      "Processed batch 28500/34897...\n",
      "Processed batch 29000/34897...\n",
      "Processed batch 29500/34897...\n",
      "Processed batch 30000/34897...\n",
      "Processed batch 30500/34897...\n",
      "Processed batch 31000/34897...\n",
      "Processed batch 31500/34897...\n",
      "Processed batch 32000/34897...\n",
      "Processed batch 32500/34897...\n",
      "Processed batch 33000/34897...\n",
      "Processed batch 33500/34897...\n",
      "Processed batch 34000/34897...\n",
      "Processed batch 34500/34897...\n",
      "\n",
      "✅ Success! Vector Store created and saved.\n",
      "You can now find the database files in: c:\\Users\\Maireg\\Documents\\GitHub\\creditrust-complaint-rag\\vector_store\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# ==========================================\n",
    "# 1. Load Processed Data\n",
    "# ==========================================\n",
    "INPUT_PATH = '../data/processed/filtered_complaints.csv'\n",
    "VECTOR_DB_PATH = '../vector_store'\n",
    "\n",
    "print(\"Loading filtered dataset...\")\n",
    "df = pd.read_csv(INPUT_PATH)\n",
    "print(f\"Total records available: {len(df)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. Stratified Sampling (Crucial Step)\n",
    "# ==========================================\n",
    "# We want a balanced dataset so the AI knows about all products equally.\n",
    "# We will take 3,000 complaints from each of the 4 categories = 12,000 total.\n",
    "\n",
    "SAMPLES_PER_CATEGORY = 3000\n",
    "sampled_df = df.groupby('Unified_Product', group_keys=False).apply(\n",
    "    lambda x: x.sample(min(len(x), SAMPLES_PER_CATEGORY), random_state=42)\n",
    ")\n",
    "\n",
    "print(f\"\\nStratified Sample Size: {len(sampled_df)}\")\n",
    "print(sampled_df['Unified_Product'].value_counts())\n",
    "\n",
    "# ==========================================\n",
    "# 3. Document Preparation & Chunking\n",
    "# ==========================================\n",
    "# We convert dataframe rows into LangChain \"Document\" objects.\n",
    "\n",
    "print(\"\\nPreparing documents...\")\n",
    "\n",
    "documents = []\n",
    "\n",
    "for index, row in sampled_df.iterrows():\n",
    "    # We include metadata so the AI knows which product/company the text belongs to\n",
    "    metadata = {\n",
    "        \"complaint_id\": row['Complaint ID'],\n",
    "        \"product\": row['Unified_Product'],\n",
    "        \"company\": row['Company'],\n",
    "        \"issue\": row['Issue'],\n",
    "        \"date\": row['Date received']\n",
    "    }\n",
    "    \n",
    "    # Create the document\n",
    "    doc = Document(\n",
    "        page_content=row['cleaned_narrative'],\n",
    "        metadata=metadata\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "# Chunking: Break long complaints into smaller pieces (500 chars)\n",
    "# This ensures the AI reads focused segments rather than getting lost in long rants.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50, # Slight overlap to maintain context between chunks\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "print(f\"Splitting {len(documents)} documents into chunks...\")\n",
    "chunked_docs = text_splitter.split_documents(documents)\n",
    "print(f\"Total chunks created: {len(chunked_docs)}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Embedding & Indexing (The Slow Part)\n",
    "# ==========================================\n",
    "print(\"\\nInitializing Embedding Model (all-MiniLM-L6-v2)...\")\n",
    "# This downloads a small, fast model optimized for semantic search\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "print(f\"Creating Chroma Vector Store at {VECTOR_DB_PATH}...\")\n",
    "print(\"This may take 5-10 minutes depending on your CPU. Please wait...\")\n",
    "\n",
    "# Create and persist the database\n",
    "# vector_store = Chroma.from_documents(\n",
    "#     documents=chunked_docs,\n",
    "#     embedding=embedding_model,\n",
    "#     persist_directory=VECTOR_DB_PATH\n",
    "# )\n",
    "\n",
    "# Optimization: Process in batches to avoid memory issues on Windows\n",
    "batch_size = 500\n",
    "total_chunks = len(chunked_docs)\n",
    "\n",
    "# Initialize empty DB\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"complaints_rag\",\n",
    "    embedding_function=embedding_model,\n",
    "    persist_directory=VECTOR_DB_PATH\n",
    ")\n",
    "\n",
    "for i in range(0, total_chunks, batch_size):\n",
    "    batch = chunked_docs[i:i + batch_size]\n",
    "    vector_store.add_documents(batch)\n",
    "    print(f\"Processed batch {i}/{total_chunks}...\")\n",
    "\n",
    "print(\"\\n✅ Success! Vector Store created and saved.\")\n",
    "print(f\"You can now find the database files in: {os.path.abspath(VECTOR_DB_PATH)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
